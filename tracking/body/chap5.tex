% !Mode:: "TeX:UTF-8"

\chapter{联合检测和数据关联的实时在线多目标跟踪方案}
\label{chap:jdan}

% 翻译参考：https://www.pianshen.com/article/20191669270/
\section{引言}
% 摘要
%近年来目标检测方法和数据关联方法取得了巨大的进步，这两种子任务对于一阶段在线目标跟踪必不可少。
%但是传统上这两个分离的模块是分别进行处理和优化，这导致了动态开放的模型设计，并需要冗余的模型参数需要学习。
%除此之外，这个领域中很少关注将两个子任务整合成一个端到端的模型来优化模型。
%在研究中，提出了一个端到端的检测关联网络，训练和推断都是在同一个网络模型中。
%检测关联网络的所有网络层都是可微的，并联合进行优化来学习有区分性的实体特征，同时使用网络输出的分配矩阵来进行鲁棒的多目标跟踪。
%模型直接使用检测和多目标跟踪的真实值所得到的损失来进行模型优化。
%所提出的方法在几个多目标跟踪的数据集上进行评估，与最好的方法相比取得了较好的跟踪性能。

% 引言
% 多目标跟踪简介
近年来目标检测方法和多目标跟踪方法都取得了巨大的进步~\cite{RN1002,RN1215,mahmoudi2019multi}。
在实际的许多应用中都会从多目标跟踪解决方案中受益，比如智能驾驶~\cite{auto_driving}、视频监控~\cite{deep_sort}、行人动作识别~\cite{mot16}等。
目前为了能在视频序列中进行多目标跟踪，按照处理流程可以将主流方法粗略分为两阶段方法和一阶段方法。


%\begin{figure*}[ht]
%	\centering
%	\includegraphics[width=0.7\textwidth]{./figures/C5Fig/end-to-end.pdf}
%	\vspace{0.2em}
%	\caption{端到端的目标检测和数据关联}
%	\label{fig:jdan_end-to-end}
%\end{figure*}

% 两阶阶段方法
两阶段方法~\cite{fang2018recurrent,nonlocal_association,poi} 包括两个互相分离的阶段，第一阶段的目标检测首先在当前视频帧中定位所跟踪的目标的位置和大小，然后在第二阶段的数据关联中抽取目标的再识别特征用于关联当前跟踪目标和历史的的轨迹段。
目前目标检测~\cite{faster,point,redmon2018yolov3}、再识别~\cite{k_reciprocal,expanded_re} 和数据关联~\cite{nonlocal_association}~的研究已取得了巨大的进步，同时也提高了多目标跟踪任务的性能。
然而由于两阶段模型目标特征的提取进行了两次，该方法在实际跟踪应用中并不能达到实时性的要求。

% 一阶段方法
% 两阶段
不同于两阶段方法，一阶段方法~\cite{jde,voigtlaender2019mots} 尝试将在线检测和关联两者集成到一个框架中，
如图~\ref{fig:jdan_consistency}（b）所示，两个子任务可以在目标表征提取中共享模型参数，以降低跟踪成本~\cite{jde,memory_improved}。
然而，几个明显的缺点阻碍了端到端多目标跟踪模型的实现。
首先，与图~\ref{fig:jdan_consistency}（a）所示的两阶段方法相比，目标检测和数据关联之间存在形态差异。
阶段一只涉及单张图像空间信息的处理，阶段二涉及时间序列上的数据关联。
这些差异使得端到端多目标跟踪模型的设计更加困难。
% 一阶段
其次，常见的一阶段方法采用独立的处理模式进行检测和关联，包括训练有效的检测模型，然后使用复杂的关联技巧来生成轨迹。
关联结果很大程度上取决于检测器的精度。
换句话说，检测和关联在训练过程中是相互独立的，
并且无法实现端到端的训练。
导致目标检测的误差会传播到关联阶段，从而降低了多目标跟踪的准确性。
% 端到端训练的数据问题
最后，对于如图~\ref{fig:jdan_consistency}（a）第一阶段的离线检测模块，多目标跟踪数据集中现有的检测结果或标签没有对应的检测网络模型参数用于构建端到端的检测跟踪模型，即检测网络的输出和关联网络的输入之间的边界框不一致阻止了整个端到端多目标跟踪模型中的训练过程。
因此，必须实现两个子模块之间的数据一致性。
此外，随着检测子模块的训练过程的继续，第二阶段预测的边界框没有相应的真实关联标签。

%而一阶段方法~\cite{jde,voigtlaender2019mots}~是在单个网络中执行目标检测和目标跟踪。
%因此，两个子任务可以在目标表征提取中共享模型参数，显著降低多目标跟踪的成本~\cite{jde,memory_improved}。

\begin{figure*}[ht]
	\centering
	\includegraphics[width=1.0\textwidth]{./figures/C5Fig/consistency.pdf}
	\vspace{0.2em}
	\caption{两阶段、一阶段和端到端方法的对比}
	\label{fig:jdan_consistency}
\end{figure*}


受上述分析的启发，本文提出了一个联合检测和关联网络（Joint Detection and Association Network，JDAN）的端到端训练框架来解决上述问题。
该框架主要由三部分组成：检测子模块、联合子模块和关联子模块。
具体来说，首先使用预训练的双流检测网络来提取初始目标候选及其表征。
然后，使用连接子模块来合并两帧之间所有可能的表征组合，以生成混淆张量。
最后，关联子模块将张量转换为关联矩阵，它表示来自两个帧的多个目标之间的匹配关系。
要联合训练前面的子模块，一个挑战在于不一致的目标问题，
与多目标跟踪任务的跟踪真实标签相比，检测子模块可能会在预测目标的位置和大小上和关联子模块所需要的数据不一致。
为了弥合这一差距，所提出的方法放弃了现有的真实跟踪标签，
如图~\ref{fig:jdan_consistency}（c）所示，利用传统的关联方法~\cite{welch1995introduction} 为检测结果生成伪标签，
然后将其输入到关联子模块以生成跟踪结果。
因此，所提出的模型可以以端到端的方式联合训练所有子模块，以生成稳健的一阶段模型。
此外，由于伪标签仅用于训练阶段而不是测试，因此它们对推理阶段的预测速度没有影响。


与之前的一阶段方法不同，所提出的方法可以以端到端的方式联合训练检测子模块和关联子模块，达到了缓解误差传播的目的。
在 MOT15~\cite{mot15} 和 MOT17~\cite{mot16} 数据集上评估所提出的方法，
并发现所提出的方法优于多个在线多目标跟踪器。
除了精度高之外，端到端方法简单且效率高，适合于实时场景的应用。
相信这项研究将对一阶段在线多目标跟踪有很好的启发作用。


总而言之，该工作的主要贡献如下：
\begin{itemize}
	\item 提出了一个端到端的架构来联合处理目标检测和关联，以缓解检测误差的传播问题。 
	该工作是第一次尝试为多目标跟踪任务进行端到端的模型训练。
	\item 所提出的方法使用伪标签来解决对象不一致问题，并提出了一个连接子模块并进行关联预测，并基于这些伪样本产生精确的跟踪结果。
	\item 通过消去研究在多目标跟踪基准数据集上进行了大量实验。
	结果表明，与几种流行的模型相比，所提出的方法可以实现实时在线跟踪，并实现具较好的跟踪精度。
\end{itemize}


\section{相关工作}
本章总结最近多目标跟踪中所取得的进展，将其分为两阶段方法和一阶段方法进行介绍，并分析了这些方法和之前所提方法的优缺点。
 
\subsection{两阶段方法}
传统的多目标跟踪方法~\cite{deep_sort,mahmoudi2019multi,zhou2018online} 通常将目标检测和数据关联作为两个独立的步骤。
首先，利用目标检测器~\cite{he2017mask,redmon2018yolov3} 以边界框的形式找出每一帧中所有的目标，并在原始图像帧中裁剪出检测结果。
第二阶段通常采用一般的数据关联方法，根据检测结果的交并比和外观表征计算相似度矩阵，
然后在视频帧之间进行状态估计~\cite{multi_pattern,local_sparse,dynamic_fusion} 和数据关联~\cite{kuhn1955hungarian,zhou2018online}，以产生各个目标的运动轨迹。
已有许多研究~\cite{mahmoudi2019multi}~利用诸如图匹配~\cite{zhou2018online}、循环神经网络~\cite{fang2018recurrent} 等最新的数据关联方法。

两阶段方法的优势在于它能针对每一阶段分别利用最合适的方法来尽可能提高跟踪性能。
除此之外，两阶段多目标跟踪方法根据检测框裁剪视频帧，并在抽取目标特征之前将目标缩放成相同的大小。
这个缩放操作能较好的解决跟踪目标之间的尺度差异。
最终，这个方法~\cite{poi}在多目标跟踪基准数据集上取得了很好的跟踪效果。
然而，两阶段方法由于在目标检测中的特征抽取和目标跟踪中的特征抽取都非常耗时，在没有模型参数共享时该方法非常慢。
因此该方法很难达到实际场景中所需的实时性要求。

\subsection{一阶段方法}
随着深度学习中目标检测、多目标跟踪和多任务学习~\cite{ranjan2017hyperface,kokkinos2017ubernet} 的发展，多目标跟踪研究的一个趋势是将目标检测和目标跟踪组合在一个单独的处理框架中。
主要的思想是在一个单独的模型中利用参数共享减少模型的运行时间，以达到同时进行目标检测和数据关联。
例如，TrackR-CNN~\cite{voigtlaender2019mots} 在 Mask-RCNN~\cite{he2017mask} 的基础之上添加了一个再识别分支来预测边界框和目标特征。
基于 YOLOv3~\cite{redmon2018yolov3} 的 JDE~\cite{jde} 获得了接近视频帧率的跟踪速度。
FairMOT~\cite{fairmot} 发现基于基于锚框的检测器预测出的目标边界框可能会和实际的目标中心没有对齐，这将会产生严重的歧义和许多的身份切换。

然而目前的一阶段多目标跟踪方法没有实现完全端到端的模型，仍然会导致检测器的误差传播到数据关联步骤，不能进行两个任务的联合优化，
为了进一步解决这个问题，该工作提出了一个联合目标检测和目标关联的真正端到端的跟踪方法，在某种程度上提高了在线多目标跟踪的精度和速度。


\section{端到端跟踪框架}
为了介绍所提出的算法，首先介绍网络的处理流程，
然后描述检测子模块的信息，
再介绍所提出的连接子模块和保持数据一致性的策略。
最后，提出关联子模块和在线跟踪策略来进行端到端多目标跟踪。
本章使用了以下符号。

\begin{itemize}
	\item $ A $ 表示关联矩阵，它指定当前帧 $F_t$ 中所有目标与历史帧 $F_{t-n}$ 中所有目标之间的关联概率。
	\item $B_{t-n, t}$ 是作为历史轨迹真实标签和当前帧真实标签之间的二进制关联矩阵。
	\item $D$ 表示偏移头的输出。
	\item $E$ 表示表征头生成的表征图。
	\item $F$、$F_t$ 和$F_{t-n}$ 分别代表任意帧、当前帧和前 $n$ 帧。
	\item $M_t$, $M_{t-n}$, $M_{t-n,t}$ 和 $M_a$ 分别表示当前帧中的表征张量，前 $n$ 帧中的表征张量，当前帧和前 $n$ 帧之间的混淆张量，以及关联矩阵。
	\item $N_m$ 表示每个视频帧中跟踪目标的最大数量。
	\item $R_t$ 和 $R_{t-n}$ 分别是当前帧和前 $n$ 帧中的表征矩阵。
	\item $S_D$、$S_J$ 和 $S_A$ 分别代表目标检测子模块、检测跟踪连接子模块和数据关联子模块。
\end{itemize}


\subsection{方法流程}
本章所提出的联合检测和关联的多目标跟踪流程如图~\ref{fig:jdan_pipeline} 所示。
由分隔 $n$ 个时间戳的一对视频帧 $F_t$ 和 $F_{t-n}$ 被输入主干网络中。
两个输入视频帧被共享参数的双流检测网络所处理，其中每个流是一个检测子模块，通过它们来学习鲁棒且高分辨率的目标表征。
%
骨干网络中的数字表示相对于原始特征的比例，
后面附加了定位头和表征头，用于预测目标边界框和目标表征 $R_t$。
%$R_t$ 是为关联子模块学习的，如 章节~\ref{sec:association_submodule} 中所述。
%
所有目标表征都连接起来形成表征矩阵 $R_t \in \mathbb{R}^{128 \times N_m}$ 和 $R_{t-n} \in \mathbb{R}^{128 \times N_m}$（没有目标的位置占位符用零进行填充），其中 $N_m$ 是输入帧中所允许的最大目标的数量。
然后，将 $R_t$ 和 $R_{t-n}$ 分别沿垂直和水平方向复制 $N_m$ 次，形成 $M_t \in \mathbb R^{128 \times N_m \times N_m}$ 和 $M_ {t-n} \in \mathbb R^{128 \times N_m \times N_m}$。
这些表征张量 $M_t$ 和 $M_{t-n}$ 的一对一组合被连接成一个混淆张量 $M_{t-n,t} \in \mathbb R^{(128+128) \times N_m \times N_m}$。
随后，使用关联预测器将 $M_{t-n,t}$ 转换为关联矩阵 $M_a \in \mathbb R^{N_m \times N_m}$。
同时，通过使用所获得的关联矩阵来回顾历史视频帧并执行在线多目标跟踪。
此外，利用现有的目标检测和数据关联方法来设计满足要求的子模块。
如图~\ref{fig:jdan_consistency}~所示，为了利用检测子模块，对应于基准数据集的未知检测网络实现与提出的检测子模块之间存在许多不一致。
为了解决检测子模块的输出和关联子模块的输入之间边界框位置和大小不一致的问题，首先在训练的第一阶段中训练检测子模块，
然后使用经过训练的检测子模块生成所有边界框，
并利用传统的关联方法在视频中产生轨迹。
因此，在训练的第二阶段中，通过固定检测子模块的参数，可以利用上一步的输出来训练所提出的关联子模块。
然后继续阶段一和阶段二的循环训练迭代，直到损失函数收敛。
最后，在线多目标跟踪是通过使用关联子模块的输出将当前帧与几个单独的历史帧相关联来执行的。
因此，这些策略可以实现两个拆分子模块之间的数据一致性，以完成训练和端到端实时检测跟踪。


\begin{figure*}[ht]
	\centering
	\includegraphics[width=1.0\textwidth]{./figures/C5Fig/pipeline.pdf}
	\vspace{0.2em}
	\caption{联合检测关联的网络架构图}
	\label{fig:jdan_pipeline}
\end{figure*}

\subsection{目标检测子模块}
\label{sec:detection_submodule}
检测子模块将单个视频帧 $F \in \mathbb{R} ^ {W \times H \times 3}$ 作为输入并获得每个视频帧的目标边界框和相应的表征。
特别是，在主干模型中添加了两种类型的预测头。
使用定位头来定位目标边界框。
此外，如图~\ref{fig:jdan_pipeline} 所示，表征头用于计算目标表征，
将其输入到 JDAN 的后半部分以获得每对视频帧的关联矩阵 $M_a$。

\subsubsection{主干网络}
\label{sec:backbone}
主干网络对于多目标跟踪任务至关重要，因为目标表征需要同时利用低分辨率和高分辨率表征来适应各种尺度的跟踪目标。
FairMOT~\cite{fairmot} 注意到深层聚合有利于减少一阶段方法的身份切换次数，因为编解码网络可以有效处理不同的目标大小。
然而，深层聚合在两阶段方法中并不重要，因为边界框通过裁剪和缩放将具有相同的大小。

在本研究中，为了同时考虑模型复杂性和精度，采用了 ResNet-34~\cite{resnet}。
如图~\ref{fig:jdan_pipeline} 所示，利用深层聚合~\cite{point} 的变体作为检测子模块的主干来适应各种尺度的目标。
与原始深层聚合~\cite{dla} 相比，它在低层和高层表征之间有额外的旁路。
另外，上采样过程中的所有卷积块都被可变形卷积模块~\cite{deformable}所替换，因为它可以自适应地适应目标尺寸的变化，
这些修改同样有利于缓解对齐问题。


\subsubsection{定位头}
\label{sec:detection_head}
定位头的输入是骨干网络的输出表征。
每个定位头使用大小为 $3\times3$ 的卷积核和 $256$ 输出通道，然后是 $1\times1$ 卷积以产生定位输出。
具体来说，它会生成一个低分辨率的位置和大小。

首先，使用热力图头来预测目标中心。
当它与真正的中心目标位置重叠时，该头部在某个位置的输出为 $1$。
输出值随着到目标中心位置的距离增加而减小~\cite{cornernet}。
对于视频帧中的真实边界框 $b^i = (x_1^i,y_1^i,x_2^i,y_2^i)$，目标的中心位置为
$ p^i = (\frac{x_1^i+x_2^i}{2}, \frac{y_1^i+y_2^i}{2})$。
因此，通过将中心位置除以下采样因子来计算表征图上的位置：$q^i = \lfloor \frac{p^i}{G} \rfloor $，其中 $G=4$。
形式上，位置 $q \in \mathbb{R}^2$ 处的热图响应定义为：
$r_{q} = \mathop{max}\limits_{i} ( \mathrm{exp}^{-\frac{(q - q^i)^2}{2\sigma ^2}} ) $，
其中 $\sigma$ 是高斯核，它是目标大小的函数~\cite{cornernet}。
根据焦点损失~\cite{lin2017focal} 设计热图损失函数 $ \mathcal{L}_{h} $ 作为训练目标：
\begin{align}
\mathcal{L}_{h} = -\frac{1}{N} \sum _{q} \begin{cases} (1-\hat{r}_{q})^\alpha \text{log}(\hat{r}_{q}), & \text{如果 } r_{q}=1 \\ (\hat{r}_{p})^\alpha \text{log}(1-\hat{r}_{q}) (1-r_{q})^\beta, & \text{否则}
\end{cases}
\end{align}
其中 $N$ 表示当前帧中目标的数量，$\hat{r}_{q} \in [0,1]^{\frac{W}{G} \times \frac{H}{ G} \times C_h}$ 是位置 $q$ 处的预测热图响应，类别号 $C_h=1$ 和 $\alpha, \beta$ 是焦点损失的超参数。

尺寸头用于预测目标围绕其中心位置的宽度和高度。
尺寸头的输出定义为： $\hat{Z} \in \mathbb{R}^{\frac{W}{G} \times \frac{H}{G} \times C_z} $，其中类别号 $C_z=2$ 表示宽度和高度。
虽然定位精度与目标表征没有直接的关系，但它会影响检测子任务的性能。
对于视频帧中的一个真实框 $b^i$，可以根据 $z^i = (x_2^i-x_1^i, y_2^i-y_1^i)$ 得到框的大小，
并且预测的边界框大小定义为 ${\hat{z}}^i$。

此外，FairMOT~\cite{fairmot} 表明具有中心位置的细化边界框对于提高多目标跟踪精度很重要。
骨干网络中的下采样因子 $G$ 将发挥巨大的量化效果。
偏移头用于更准确地检测目标。
虽然检测精度提升的优势微乎其微，但是多目标跟踪中的目标表征是基于极其精确的边界框学习的，因此在这里引入偏移头，
将偏移头的输出表示为 $\hat{D} \in \mathbb{R}^{\frac{W}{G} \times \frac{H}{G} \times C_d} $，其中类别号 $C_d=2$。
表征图上的真实位移表示为： $d^i = \frac{p^i}{G} - \lfloor \frac{p^i}{G} \rfloor $。
将中心位置位移表示为 ${\hat{d}}^i$。
因此，表示尺寸头和偏移头的 $L_1$ 损失表示为：
\begin{equation}
\mathcal{L}_{s} = \frac{1}{N} \sum_{i=1}^{N} \|z^i - \hat{z}^i\|_1 + 
\frac{1}{N} \sum_{i=1}^{N} \|d^i - \hat{d}^i\|_1.
\end{equation}

因此，定位损失 $\mathcal{L}_{p}$ 表示为前两个损失的组合：
\begin{equation}
\mathcal{L}_{p} = \mathcal{L}_{h} + \mathcal{L}_{s}.
\end{equation}


\subsubsection{表征头}
表征头的目的是提取可以区分各种跟踪目标的外观表征。
在理想情况下，不同身份的目标之间的差异大于同一身份目标之间的差异。
为了实现这一目标，骨干网络的输出为检测目标的表征，
生成的表征图为 $E \in \mathbb{R}^{\frac{W}{S} \times \frac{H}{S} \times C_e}$，其中输出通道 $C_e=128$。
通过表征头学习在中心位置 $p$ 的目标的表征 $E_{p}\in\mathbb{R}^{C}$。
将跟踪目标识别视为分类问题。
同时训练数据集中所有相同身份的目标都被视为一个标签。
对于视频帧中的真实框 $b^i$，获得了热图上的目标中心位置 $\hat{p}^i$。
在某个位置学习一个身份表征 $E_{p^i}$ 并输出到一维分类概率向量 $v(k)$，
并将真实分类标签表示为 $u^i{(j)}$。
因此，身份分类损失被构造为：
\begin{equation}
\mathcal{L}_{c} = \frac{1}{N \times J} \sum_{i=1}^{N} \sum_{j=1}^{J} u^i{(j)} \text{log}(v(j)),
\end{equation}
其中 $J$ 是数据集中所有身份的数量。

最后，总的检测损失 $\mathcal{L}_{d}$ 表示为前两个损失的组合：
\begin{equation}
\mathcal{L}_{d} = \mathcal{L}_{p} + \mathcal{L}_{c}.
\end{equation}



\subsection{连接子模块和数据一致性}
JDAN 训练输入是没有目标边界框的当前帧 $F_t$ 和历史帧 $F_{t-n}$。
此外在关联子模块 $S_A$ 的训练中，JDAN 需要一个真实的二进制关联矩阵 $B_{t-n, t}$ 作为历史帧和当前帧之间的真实标签来计算关联损失。
在图~\ref{fig:jdan_pipeline}~的最左边显示了一对 JDAN 的输入图像帧。
下面描述连接子模块的细节和所需要解决的数据一致性问题。


\subsubsection{连接子模块}
在目标检测子模块 $S_D$ 和关联子模块 $S_A$ 之间，所提出的连接子模块 $S_J$ 将当前帧中的目标表征 $R_t$ 沿垂直方向复制到张量 $M_t \in \mathbb{R}^{128 \times N_m \times N_m}$，
并将历史帧中的目标表征 $R_{t-n}$ 沿水平方向复制到张量 $M_{t-n} \in \mathbb{R}^{128 \times N_m \times N_m}$。
随后如图~\ref{fig:jdan_pipeline} 所示，目标表征 $M_t$ 和 $M_{t-n}$ 沿着目标表征的通道方向合并到 $M_{t-n,t} \in \mathbb{R}^{(128 + 128) \times N_m \times N_m}$。
注意到，垂直和水平复制用于尽可能多地将两组目标进行排列组合，这确保了历史帧 $F_{t-n}$ 中的目标可能与当前帧 $F_t$ 中的所有目标相关联，反之亦然。
然后通过包含的关联预测器五个卷积块~\cite{inception} 将扩展的混淆矩阵 $M_{t,t-n}$ 转换为关联矩阵 $M_a \in R^{N_m \times N_m}$。
在表~\ref{tab:compression_net} 中详细描述了关联预测器的有关信息。
%I.C 是每一层的通道数目, 
%O.C 表示输出通道的数目, 
%$Stride$ 表示步长的大小, 
卷积核是 $1 \times 1$ 的卷积核来压缩维度，卷积核的大小表示感受野的大小； 
BN (Y/N) 表示是否使用批量正则化；
ReLU (Y/N) 表示是否使用 ReLU。
步长和填充在空间维度上都是相同的，卷积核的步长代表提取的精度。

\begin{table}[t]
	\centering
	\tabcolsep=3.5pt
	\caption{关联预测器压缩网络框架的详细信息}
	\label{tab:compression_net}
	\tabcolsep=0.15cm
	\begin{tabular}{c|cccccccc}
%		\hline
		\hline
%		\toprule[1.5pt]
		{子模块}	&{索引} &{输入通道数} &{输出通道数} &{卷积核} &{步长} & {填充} &{BN} &{ReLU} \\
		\hline
%		\midrule[1.5pt]
		\multirow{2}{*}{}
		&	1     & 1024  & 512  	& $1 \times 1$ 	& 1 & 0 &	Y	&	Y\\
		&	2     & 512   & 256   	& $1 \times 1$	& 1 & 0 &	Y	&	Y\\
		\multirow{1}{*}{关联预测器}
		&	3     & 256   & 128   	& $1 \times 1$ 	& 1 & 0 &	Y	&	Y\\
		\multirow{1}{*}{}
		&	4     & 128   & 64   	& $1 \times 1$ 	& 1 & 0 &	N	&	Y\\
		&	5    & 64    & 1    	& $1 \times 1$ 	& 1 & 0 &	N	&	Y\\
%		\bottomrule[1.5pt]
		\hline
%		\hline
	\end{tabular}%
\end{table}%


\subsubsection{训练数据的连接一致性}
在连接子模块中，所有目标表征 $R_t$、$R_{t-n}$ 都来自检测子模块，并且可能与多目标跟踪基准数据集上的跟踪真实值存在数据不一致。
因此，很难进行端到端的模型训练。
为了解决这个问题，在该研究中不使用数据集中跟踪的真实值，采用了一种简单而有效的传统关联方法，称为卡尔曼滤波器~\cite{welch1995introduction}来预测轨迹的位置，从而生成目标表征 $R_t$ 和 $R_{t-n}$ 之间的伪关联标签。
根据伪标签得到一个伪关联矩阵 $B_{t-n,t} \in \mathbb{R}^{(N_m+1) \times (N_m+1)}$，其中每个元素 $b_{k,l}$ 表示目标 $k$ 和 $l$ 之间的匹配关系，增加一列/行（$B_{t-n,t}$记为“+1”）表示对象消失/出现在当前帧中。
为 $b_{k,l}$ 定义了三个值：$1$ 表示目标 $k$ 和 $l$ 之间的相同身份（称为“伪正对”），$0$ 表示不同的身份（称为“伪负对”） ")，$0.5$ 表示不确定。
在具体的实现中，为卡尔曼滤波器中设置了高阈值以减少伪正对的错误匹配，并设置低阈值以增加伪负对的真实不匹配，余下的配对设置为不确定。

%多目标跟踪基准数据集中的真实边界框和所提出的的检测子模块的输出结果在位置和大小上不一致，
%并且在端到端多目标跟踪中所需的多目标跟踪基准数据中的相应检测器模型参数也无法获取到。
%因此如图~\ref{fig:jdan_consistency}所示，利用经过训练的检测子模块提供的边界框和身份轨迹信息以及多目标跟踪基准数据集中的传统关联方法来解决如的真实值边界框不一致的问题。
%有许多不一致的边界框，包括边界框真实值~\cite{dpm,faster,sdp}和所提出的的检测模块在多目标跟踪基准数据集中的输出之间的数量和位置。
%虽然多目标跟踪基准数据集中提供的训练数据缺乏这些检测模型的实现，但有必要实现一个阶段的多目标跟踪，如章节~\ref{sec:two_stage} 和一阶段多目标跟踪。
%
%首先，使用预训练的检测子模块和传统的关联方法在多目标跟踪数据集中生成一系列轨迹。
%然后，利用这些轨迹结果形成二进制关联矩阵 $B_{t-n,t}$ 跟随 章节~\ref{sec:similarity_loss}，
%然后利用它来训练关联子模块。
%最后，使用两阶段训练策略，如章节~\ref{sec:two_stage} 中描述的那样训练整个JDAN，在检测子模块的输出和关联子模块。


\subsection{关联子模块}
\label{sec:association_submodule}
JDAN 中的目标关联子模块 $S_A$ 的目的是使用连接的混淆张量计算 $F_{t-n}$ 和 $F_t$ 这两个目标组之间的关联 $M_{t-n,t}$。

\subsubsection{关联预测器}
\label{sec:similarity_estimator}
如表~\ref{tab:compression_net}所示，关联预测器的结构是根据 $M_{t-n,t}$ 和 $M_a$ 的实际含义设计的。
该模块将目标表征的组合 $M_{t-n,t}$ 转换为关联矩阵 $M_a$，表示这些帧间跟踪目标的关联信息~\cite{dan}。
因此，它沿着目标表征的方向使用卷积核大小为 $1\times 1$ 的卷积逐步实现了从 $256$ 到 $1$ 的维度压缩，同时它不会相互影响特征图中的相邻通道。



\begin{figure*}[ht]
	\centering
	\includegraphics[width=1.0\textwidth]{./figures/C5Fig/loss.pdf}
	\vspace{0.2em}
	\caption{目标消失和出现的处理}
	\label{fig:jdan_loss}
\end{figure*}


\subsubsection{关联矩阵} \label{sec:association_matrix}
如图~\ref{fig:jdan_pipeline} 的后半部分所示，通过利用所提出的关联子模块获得帧间关联，并利用每帧中允许的最大目标数 $N_m$ 预测 $F_{t-n}$ 和 $F_t$ 之间的目标关联矩阵 $M_a$。
如章节~\ref{sec:maximum_object} 中所述，在本研究中 $N_m = 150$ 是多目标跟踪数据集的单帧中目标数目上限。
沿水平和垂直方向在目标相似性关联矩阵 $M_a$ 中插入零向量（作为目标占位符）以进行泛化。
这些零出现在 $F_{t-n}$ 和 $F_t$ 之间，因此任何视频帧最终都由 $N_m$ 个目标组成，并且 $M_a$ 的形状是 $N_m \times N_m$。

$M_a$ 中的行表示历史帧中的目标，其中的列表示当前帧中的目标。
$M_a$ 表示具有水平和垂直目标占位符的两个视频帧的关联矩阵。
在 $M_1$ 和 $M_2$ 中，在末尾矩阵附加了一个额外的水平和垂直向量，称为未识别的目标~\cite{dan}。
如图~\ref{fig:jdan_loss} 所示，最后附加的垂直向量负责建模从历史帧 $F_{t-n}$ 中消失的当前跟踪目标，最后一行中附加的水平向量负责建模在当前帧 $F_t$ 中进入视野的新目标。
对于输入的历史帧和当前帧，JDAN 预测得到相似性关联矩阵 $M_a$。
考虑到历史帧和当前视频帧之间的多目标消失和出现，通过向 $M_a$ 添加额外的列和行来设计 $M_1$ 和 $M_2$。
然后，分别对 $M_1$ 和 $M_2$ 进行水平和垂直 softmax 操作，以保证出现和消失的总概率都为 1，裁剪后的矩阵 $A_c$ 和 $A_r$ 用于与真实关联矩阵计算损失 $ L_m $。
最后，总关联损失 $\mathcal{L}_s$ 由 $\mathcal L_m$、$\mathcal L_b$ 和 $\mathcal L_d$ 所构成。
所以所提出的网络可以表示相机视野中多个目标消失和出现。
例如，可以在最后附加的垂直向量中的一行，从 1 变成 0 来表示消失，
并在最后附加的水平向量的列处，从 1 变成 0 表示出现。



\subsubsection{消失和出现} \label{sec:similarity_loss}
可以计算预测的关联矩阵 $M_a$ 和真实的二进制关联矩阵 $B_{t-n,t} \in \mathbb{R}^{(N_m+1) \times ( N_m+1)}$。
关联矩阵的标签最终用于训练所提出的的关联子模块 $S_A$。
然而，$M_a$ 忽略了历史和当前帧之间的目标消失和出现。
因此，利用历史帧和当前帧之间的相似性关联编码来考虑多个目标的消失和出现。

如图~\ref{fig:jdan_loss} 顶部所示，考虑到目标消失，在 $M_a$ 后附加一列以构建 $M_1 \in \mathbb R^{N_m \times (N_m + 1)}$。
扩展矩阵 $M_{1}$ 的第 $m^{\text{th}}$ 行将帧 $F_{t-n}$ 中的 $m^{\text{th}}$ 目标与 $F_t$ 帧中 $N_m+1$ 个的目标进行关联，其中 $+1$ 表示当前帧 $F_t$ 中未检测到的目标。
然后，通过执行 softmax 操作~\cite{train_mot} 对 $M_1$ 的水平方向上的扩展概率向量进行归一化。
因此，输出关联矩阵 $A_{1} \in \mathbb R^{N_m \times (N_m +1 )}$ 的水平向量表示视频帧 $F_{t-n}$ 中所有目标与在视频帧 $F_t$ 中的所有目标之间的关联概率，包括当前视频帧中未识别的目标。
同理如图~\ref{fig:jdan_loss}~底部所示，目标外观是通过向 $M_a$ 附加一行以构建 $ M_2 \in \mathbb R^{ (N_m + 1) \times N_m}$ 而形成的。
然后，对 $M_2$ 执行垂直 softmax 操作得到 $A_2 \in \mathbb R^{(N_m +1) \times N_m}$，其列表示来自视频帧 $F_{t-n}$ 到 $F_t$ 的关联概率~\cite{train_mot}。


此外，向目标关联矩阵 $M_a$ 添加了额外的列和行以获得可理解的损失设计。
关联矩阵 $M_a$ 添加的向量是 ${\bf u} \in \mathbb R^{N_m} = \lambda {\bf 1} $，其中 $\lambda$ 是超参数，${\bf 1}$ 是一个全为 1 的单位向量~\cite{dan}。
添加向量的这种设计意味着所有跟踪目标都有消失或出现的概率。
此外，二进制关联矩阵 $B_{t-n,t}$ 以相同的方式实现。

具体来说，使用方向性损失 $\mathcal L_{d}$ 来抑制消失和出现的错误目标关联：
\begin{align}
\mathcal L_d = \frac{\sum_{i=1}^{N_m} \sum_{j=1}^{N_m+1} \left( \left(-\log { A_1} \right) \odot {B}_1 \right)}{ \sum_{i=1}^{N_m} \sum_{j=1}^{N_m+1} B_1 }  
\notag  \\
+ \frac{ \sum_{i=1}^{N_m+1} \sum_{j=1}^{N_m} \left( \left(-\log { A_2} \right) \odot {B}_2 \right)}{\sum_{i=1}^{N_m+1} \sum_{j=1}^{N_m} B_2},
\end{align}
其中 $B_1$ 和 $B_2$ 分别通过删除 $B_{t-n,t}$ 的最后一个水平和垂直向量来进行定义，
运算符 $\odot$ 表示哈达玛积~\cite{hadamard}，
\textit{log} 函数作用于参数中的每个元素。

此外，利用非极大值损失和平衡损失来训练关联子模块~\cite{dan}。
非极大值损失 $\mathcal L_{m}$ 在关联计算的消失和出现中惩罚非最大关联：
\begin{equation}
\mathcal L_m = \frac{ \sum_{i=1}^{N_m} \sum_{j=1}^{N_m} \left( \left(-\log A_m \right) \odot {B}_3 \right)}{\sum_{i=1}^{N_m} \sum_{j=1}^{N_m} B_3}.
\end{equation}
同理，$B_3$ 是同时删除 $B_{t-n,t}$ 的最后一个垂直向量和最后一个水平向量，
$A_m = max (A_c, A_r)$。
\textit{max} 函数也会作用于输入参数的每个元素。
对损失 $L_m$ 进行 $max$ 操作以获得 $A_c$ 和 $A_r$ 中的最大值，
如图~\ref{fig:jdan_loss}~所示，其中 $A_c$ 和 $A_r$ 分别表示矩阵 $A_1$ 和 $A_2$ 通过删除最后一个垂直向量和最后一个水平向量被裁剪到 $N_m \times N_m$ 的维度。
在视频过程中出现的目标数和消失的目标数应该相等，所以平衡损失 $\mathcal L_b$ 惩罚消失和出现之间的任何不平衡：
\begin{equation}
\mathcal L_b =  \sum_{i=1}^{N_m} \sum_{j=1}^{N_m} \lvert A_c^{ij} - A_r^{ij} \rvert. 
\end{equation}

最后，将总关联损失 $\mathcal L_s$ 定义为上述三项损失的总和：
\begin{equation} \label{equ:association_loss}
\mathcal L_{s} = \mathcal L_d + \mathcal L_m + \mathcal L_b.
\end{equation} 
在这里，关联子模块的训练目标是最小化关联损失 $\mathcal{L}_s$。
因此，上述三个损失是有效的，并拟合了真实目标关联。


\subsection{端到端跟踪}
这一部分描述所提出的端到端模型的训练和用法，以及在没有输入边界框的视频序列中执行多目标跟踪的详细步骤。

\subsubsection{两阶迭代段训练}
\label{sec:two_stage}
在该研究中采用迭代训练过程来训练所提出的模型，包括两个步骤：
首先，在几个检测数据集上采用预训练的目标检测模型~\cite{zhang2017citypersons,xiao2017joint,zheng2017person}，并根据检测损失 $\mathcal{L}_{d}$ 对其参数进行微调；
其次，利用卡尔曼滤波器~\cite{welch1995introduction} 进行跟踪，得到目标的轨迹和身份标签，以及当前检测结果和历史轨迹之间的伪标签，
并根据方程~\ref{equ:association_loss} 中的 $\mathcal{L}_{s}$ 更新检测子模块和关联子模块。
反复重复上述两个步骤，直到损失 $\mathcal{L}_{s}$ 收敛。
与以前的方法相比，检测误差可以反向传播回来以更新检测子模块和关联子模块，
因此所提出的训练方法可以实现端到端模型训练以缓解误差传播的问题。

%为了验证基于之前的在线多目标跟踪研究所提出的模型 JDAN，它包含两个可训练的子模块，名为检测子模块 $S_D$ 和关联子模块 $S_A$。
%最后，执行端到端推理。
%首先如~\ref{sec:detection_submodule}节所示，使用边界框框和身份信息和检测损失 $\mathcal{L}_{d}$ 来训练定位头和表征头。
%其中一些数据没有身份信息只有真实边界框，但它们可以被用来训练定位头。
%
%然后，根据预训练的检测子模块生成的边界框对多目标跟踪数据集执行传统的数据关联方法，
%并利用输出生成二进制关联矩阵 $B_{t-n,t}$ 作为训练关联子模块 $S_A$ 的真实数据。
%在第二个训练阶段，通过 $S_D$ 固定检测子模块 $S_D$ 的参数与生成的边界框一致，
%并使用关联子模块 $S_J$ 产生的混淆矩阵和传统数据关联方法生成的二进制关联矩阵训练 $S_A$。
%由于使用了预训练的检测子模块和固定参数，第二个训练阶段和端到端推理阶段生成的表征和框是相同的。
%这些策略确保检测子模块生成的边界框和目标表征在第二个训练阶段在 $S_D$ 和 $S_A$ 之间是一致的。


\label{sec:dep}
\subsubsection{模型预测}
尽管 JDAN 中的目标检测子模块在训练时为两个并行分支，但在线多目标跟踪使用的是同一个网络。
这样做是合理的，因为两个并行检测子模块的权重相互共享。
如图~\ref{fig:tracking} 所示，所提出的网络的预测通过有序的方式呈现主要模块。
JDAN 的输入是一个视频帧 $F_t$，尺寸大小为 $1088 \times 608$。
基于根据章节~\ref{sec:detection_head} 预测出的热力图，由热图响应进行非极大值抑制操作以获得最强点，
选择热图最强响应超过极限值的位置。
然后，根据模型推断结果的大小和偏移量预测相应的框。

此外，检测子模块中的表征头学习当前帧和历史帧的目标表征 $R_t$ 和 $R_{t-n}$。
复制并组合这两个表征矩阵为这两个图像的混淆张量 $M_{t-n,t}$。
然后如章节~\ref{sec:similarity_estimator} 所示，张量 $M_{t-n, t}$ 通过关联预测器的前向传播转换为关联矩阵 $M_a$。

\begin{figure*}[ht]
	\centering
	\includegraphics[width=1.0\textwidth]{./figures/C5Fig/tracking.pdf}
	\vspace{0.2em}
	\caption{JDAN 执行在线多目标跟踪的过程}
	\label{fig:jdan_tracking}
\end{figure*}

\subsubsection{在线跟踪}
在初始化跟踪过程并在第一帧中生成表征 $R_0$ 后，可以预测出当前帧目标和响应历史帧目标表征之间的关联矩阵 $M_{t-n:t-1,t}$。
如图~\ref{fig:jdan_tracking} 所示，推断出的关联矩阵通过回顾历史帧信息来刷新先前的跟踪结果。
在输入视频帧 $F_t$ 中，通过 JDAN 中具有单个流检测子模块，并由定位头和表征头分别得到目标框和目标表征。
目标表征 $R_t$ 用于与最近的 $n$ 个历史表征 $R_{t-n:t-1}$ 进行配对，并且每对表征都通过关联预测器来估计相应的关联矩阵 $M_{t-n:t-1,t}$。
此外，表征 $R_t$ 保存到轨迹记录器中用于估计下 $n$ 帧中的关联矩阵。
最后，通过使用预测的关联矩阵将当前图像目标与 $n$ 个历史图像目标联系起来并更新轨迹记录器。

在这里描述了图~\ref{fig:jdan_tracking} 所示的详细在线多目标跟踪过程。
记录器 $T_0$ 具有相同数量的轨迹，因为识别的目标在第一个视频帧 $F_0$ 中被初始化。
此外，每条轨迹都是一个键值对的记录器，其中的每一项都包含视频帧索引和目标表征。
使用 Kuhn-Munkres 方法更新当前帧的轨迹记录器~\cite{Munkres1957}，并通过最大化当前目标和历史轨迹的关联，来进行关联推断。
此外，将其记录到累加器 $Y_{t}$ 中。
累积器矩阵中的每个元素都是历史轨迹记录器 $T_{t-1}$ 中的目标与当前帧中的目标的相似度总和。

因此，视频中的每一帧图像仅使用检测子网络提取一次特征。
然而，目标表征被保存并重复使用多次来评估与剩余图像的相似性。
所以基于关联矩阵可能将许多轨迹分配给累积器矩阵中的特定未检测目标列。
这个问题是通过复制累加器矩阵的最后一列来解决的，直到 $T_{t-1}$ 中的每个估计都分配给唯一的一列~\cite{dan}。
因此，该策略能使每个未检测到的轨迹与未检测到的目标相关联。

总之，所提出的检测跟踪器是一种在线多目标跟踪方法，它不利用任何未来信息来预测目标轨迹。
因此，它可以应用于在线应用中。
一个潜在的问题是过长的轨迹可能会导致大量的存储和计算成本。
因此，阈值 $\mu_m$ 用于限制在现有轨迹中查看的历史帧数。
如果轨道中的图像数量超过 $\mu_m$，则最远的目标信息将被丢弃。
此外，如果跟踪目标从视野中消失超过 $\mu_r$ 帧，它将从跟踪列表中删除~\cite{train_mot}。
在提出的多目标跟踪方法中使用这些有物理含义的参数，可以根据运行时内存和计算资源的限制进行更改。

 
\section{实验}
在这一节分三步展示所提出的 JDAN 实验和结果。
首先，介绍了所使用的数据集和所提出模型的实现细节。
其次，分析了不同训练方法在同一多目标跟踪数据集上的性能。
最后，将所提出的方法与经典和最新的多目标跟踪方法进行比较。

\subsection{数据集和度量标准}
基于已有的研究~\cite{jde,fairmot}，通过合并来自多个行人检测数据集的训练数据来利用一个大型训练集来训练所提出的检测子模块。
CityPersons~\cite{zhang2017citypersons} 和 ETH~\cite{ess2008mobile} 数据集仅提供框信息，仅可以使用这些数据集来训练定位头。
CUHK~\cite{xiao2017joint}、Caltech~\cite{dollar2009pedestrian}、MOT16~\cite{mot16} 和 PRW~\cite{zheng2017person} 提供了行人边界框和身份信息，便可以联合训练定位头和表征头。
最后，基于训练好的检测子模块和传统的关联方法~\cite{kernelized,fairmot}，通过执行多目标跟踪过程构建训练数据来训练目标关联子模块。

在两个不同的多目标跟踪基准数据集 MOT15 和 MOT17 上对所提出方法的各个组件进行了大量的测试。
MOT17 包含七个训练视频和七个测试视频。
MOT16 包含与 MOT17 数据集相同的视频序列。
而 JDAN 的输入是没有检测信息的纯图像。
因此，该研究中只使用 MOT17 数据集。
另外，使用标准多目标跟踪准确度（MOTA）和 多目标跟踪精度（MOTP），
而 IDF1~\cite{ristani2016performance} 综合了 ID 准确率和 ID 召回率。
评估指标还包括 ID\_Sw、ID Precision（IDP）、误报总数（FP）、遗漏目标总数（FN）和碎片轨迹总数（Frag）~\cite{clear}。
在多目标跟踪基准测试中利用这些指标来衡量多目标跟踪性能。

\subsection{实现细节}
\label{sec:implementation_details}
在该研究中使用 PyTorch 1.2.0~\cite{pytorch} 实现所提出的 JDAN，在 Quadro RTX 6000 GPU 上花费 20 小时进行训练。
使用训练数据集来训练所提出的模型，并基于 MOT17 选择所提出网络的超参数。
因为基准数据集 MOT17 的数据规模不大，选择它进行超参数调整。
最后，实验中使用的超参数描述如下。

如章节~\ref{sec:backbone} 中所述，修改后的 DLA~\cite{point} 被用作检测子模块的主干。
使用在微软 COCO~\cite{lin2014microsoft} 上训练的权重初始化检测子模块。
%输入帧缩放到 $1088\times 608$。
%提议的网络的输入帧尺寸为 $1088\times608$。
在将输入帧输入到 JDAN 之前，这些训练和测试样本被重新缩放到指定的大小，表征图的大小为 $272 \times 152$。
如图~\ref{fig:jdan_consistency} 训练阶段一所示，用 SGD 优化器~\cite{sgd} 训练检测子模块 50 次迭代，初始学习率为 $1e-4$，在第 25 和 40 次迭代时乘以 0.1。
%
每帧允许的最大目标数 $N_m$ 设置为 150，最小批次大小 $B$ 设置为 4，迭代轮次设置为 160，单位向量 $\lambda$ 的乘数因子设置为 10。
将历史帧和当前帧 $N_a$ 之间的最大间隙帧数设置为 30。
然后如图~\ref{fig:jdan_consistency} 训练阶段二所示，使用 SGD 优化器~\cite{sgd} 训练关联子模块，其动量和权重衰减分别设置为 0.9 和 5e-4。
以 0.01 的学习率开始训练，并在第 60、100 和 140 次迭代时乘以 0.1。
训练检测子模块和关联子模块时，需要优化超参数 $\mu_r$ 和 $\mu_m$。
使用网格搜索技术选择两个超参数的最佳值，以在 MOT17 验证数据集上获得最佳 MOTA 性能。
利用 $[3, 30]$ 范围内以 3 为步长来构建网格。
因此，在实际跟踪过程中使用这种方法选择了 $\mu_m=15$ 和 $\mu_r=12$。


%\subsubsection{数据增强}
\label{sec:PP}
同时利用了一系列数据增强方法，例如调整图像尺寸、裁剪和像素值抖动等。
首先，使用 1.0-1.25 范围内的随机采样率增加图像帧的大小，并用多目标跟踪训练集中的平均像素值填充扩展图像中的像素。
同时，裁剪了在 0.75-1.0 随机采样范围内的视频帧。
此外，图像中的每个像素值都乘以范围 0.75-1.25 内的随机值。
输出帧转换到 HSV 空间，饱和分量乘以范围 0.75-1.25 内的随机值。
最后参照 SSD~\cite{Liu2016} 将图像变换回 RGB 空间并乘以随机因子样本。
%
注意到历史帧 $F_{t-n}$ 和当前视频帧 $F_t$ 在视频序列中不一定是连续的，
可以让它们有 $n$ 帧的分隔，其中 $n \in [0, N_a-1] $。
然而，所提出的 JDAN 用于关联连续帧中的目标。
使用跳跃的视频帧进行训练有利于在当前帧与一系列历史视频帧之间的数据关联中使用现有的多目标跟踪方法。
以 0.25 的概率对每个轨迹上的历史和当前视频帧进行采样。
然后，这些视频帧被重新调整为指定的大小 $W \times H \times 3$。
同时，使用了概率为 0.5 的水平翻转。
此外，多目标跟踪中使用的训练数据~\cite{mot16,Lyu2017} 缺乏捕捉背景变化、相机失真和许多现实效果以保持多目标跟踪鲁棒性的能力。
在所提出的跟踪方法中，至关重要的是训练数据涉及足够多的不相关跟踪属性，以增强多目标跟踪模型的鲁棒性。
%因此，对多目标跟踪训练数据集进行后续的数据增强。
%
%所有数据增强方法都在章节~\ref{sec:implementation_details} 中描述
%修改多目标跟踪训练集的方法受到~\cite{Liu2016}的启发，他修改了视频帧以增强训练过程。
%然而，使用先前报告的数据增强方法同步处理历史和当前帧~\cite{Liu2016}。



\subsection{消去实验和讨论}

\subsubsection{训练方法}

\vspace{0.5em}
\renewcommand\arraystretch{1.5}
\begin{table}[htbp]\wuhao
	\centering
	\caption{在 MOT17 基准数据上测试各种训练配置}
	\vspace{0.3em}
	\begin{tabular}
%		{p{3.0cm}<{\centering} p{1.2cm}<{\centering} p{1.0cm}<{\centering} p{1.0cm}<{\centering}
%	p{1.0cm}<{\centering}
%	p{1.0cm}<{\centering}
%	p{1.0cm}<{\centering}
%	p{1.0cm}<{\centering}}
		{c|ccccccc}
%		\toprule[1.5pt]
%		\hline
		\hline
		方法 & MOTA$\uparrow$ & MOTP$\uparrow$ & IDF1$\uparrow$ & MT$\uparrow$ & ML$\downarrow$ &  ID\_Sw$\downarrow$ & Frag$\downarrow$\\
%		\midrule[1.5pt]
		\hline
		{基准模型} & 20.7 & 38.3 & 38.2 & 17.6 & 48.8 & 24,875 & 6,731\\
		{预训练的检测模型} & 34.6 & 42.8 & 40.4 & 19.9 & 46.7 & 18,264 & 4,084\\
		{精调的检测模型} & {42.9} & {52.8} & {48.3} & {21.5} & {45.8} & {13,236} & {3,387}\\
		{精调的关联模型} & {53.0} & {65.2} & {49.7} & {23.4} & {44.3} & {11,875} & {2,845}\\
		JDAN & {\bf58.1} & {\bf79.8} & {\bf59.2} & {\bf27.7} & {\bf32.9} & {\bf6,129} & {\bf1,515}\\
%		\hline
		\hline
%		\bottomrule[1.5pt]
	\end{tabular}
	\label{tab:jdan_training_methods}
\end{table}

在这个阶段，通过使用所提出的几个组件来评估训练过程的效果。
如表~\ref{tab:jdan_training_methods} 所示，执行了五种消去实验，包括基准模型、{预训练的检测模型}、{精调的检测模型}、精调的关联模型和所提出的 JDAN。

基准模型首先采用在微软 COCO~\cite{lin2014microsoft} 上预训练的检测子模块，然后使用生成的伪标签进行关联子模块的微调。
在此基础上，预训练的检测模型表示在检测数据集上重新训练{检测子模块}，然后使用在生成的伪标签上训练的{关联子模块}。
考虑到多目标跟踪和检测数据集之间的视觉差距，在多目标跟踪检测数据集上对检测子模块进行微调来提高检测性能，
而精调的关联模型不对{检测子模块}使用多目标跟踪数据集，而是在多目标跟踪数据集上微调{关联子模块}。
JDAN 是在多目标跟踪数据集上对{检测子模块}和{关联子模块}进行端到端训练的整个模型。

表~\ref{tab:jdan_training_methods} 报告了上述训练范式之间的性能比较结果，分析如下：
\begin{enumerate} 
	\item 与{基准模型}相比，{预训练的检测模型}实现了显著的性能提升，MOTA 从 $20.7\%$ 增加到 $34.6\%$，MOTP 从 $38.3\%$ 增加到 $42.8\%$。
	这种改进表明在检测数据集上重新训练{检测子模块}的有效性，因为微软 COCO 上的预训练无法为多目标跟踪任务中的拥挤场景提供准确的行人结果。
	
	\item {精调的检测模型}与{预训练的检测模型}相比进一步提升了 MOTA 性能，MOTP 从 $42.8\%$ 上升到 $52.8\%$，ID\_Sw 从 $18,264$ 降低到 $13,236$。
	这种改进源于在{检测子模块}上使用多目标跟踪数据集来弥合检测和多目标跟踪数据集之间的视觉差距。
	同时，由于使用多目标跟踪数据集更新{关联子模块}，{精调的关联模型}也实现了与{精调的检测模型}相同的改进。
	
	\item 从实验结果可以看出 JDAN 以最佳的效果改进了所有指标。
	这种增强不仅归功于精调的{检测子模块}，还归功于精调的{关联子模块}。
	具体来说，JDAN 在 MOTA 上达到了 $58.1\%$，这反映了跟踪的准确性。
	如前所述，JDAN 采用{端到端}训练来缓解误差传播问题，并可以获得强大的对象关联能力。
\end{enumerate}

% 
%在这个阶段，使用所提出的子模块评估两个训练阶段的效果。
%并执行各种训练配置，包括“基准模型”、“检测模型训练”、“未精调检测网络”、“未精调关联网络”和提出的 “JDAN” 训练方法。
%这些方法的组件在每种训练方法中仅更改一次。
%将 MOT17 分为 7 个训练集和 7 个验证集来训练目标关联子模块。
%这里没有使用其他更多的数据来验证所提出的训练方法的有效性。
%
%跟踪性能结果显示在表格~\ref{tab:jdan_training_methods} 中。
%在加粗字体中展示了最佳性能。
%“未训练”是基线原始模型。
%“用基础数据训练”是从除 MOT17 以外的各种数据集训练的初始模型。
%“未精调检测网络”是从 MOT17 关联数据而非 MOT17 检测数据训练的微调模型。
%“未精调关联网络” 是从除 MOT17 关联数据之外的多目标跟踪检测数据训练的微调模型。
%JDAN 是在多目标跟踪检测和关联数据上微调的整个模型。
%对“未训练” 和 “用基础数据训练”的分析表明，一定数量的数据会提升整个 MOTA 的性能。
%目标检测和关联极大地受益于更大的训练数据集。
%例如，MOTA 从 $20.7\%$ 增加到 $34.6\%$，
%和 MOTP 从 $38.3\%$ 增加到 $42.8\%$ 对于“用基础数据训练”。
%这些性能改进证明，通过使用更多的训练数据，基本数据集在提高目标检测和关联精度方面具有巨大优势。
%
%“未精调检测网络”仅在“用基础数据训练”的基础上对关联子模块进行微调，获得良好的 MOTA 性能。
%特别是，MOTP 从 $42.8\%$ 显著提高到 $65.2\%$，同时将 ID\_Sw 从 $18264$ 减少到 $11875$。
%跟踪性能表明通过使用更多的训练数据增强了联想能力。
%同时，“未精调关联网络” 与 “用基础数据训练”实现了相同的效果。


\vspace{0.5em}
\renewcommand\arraystretch{1.5}
\begin{table}[htbp]\wuhao
	\centering
	\caption{在 MOT17 数据集上评估目标表征的维度对性能的影响}
	\vspace{0.3em}
	\begin{tabular}
%		{
%			p{2.0cm}<{\centering} p{1.5cm}<{\centering} p{1.3cm}<{\centering} p{1.3cm}<{\centering}
%			p{1.3cm}<{\centering}
%			p{1.3cm}<{\centering}}
		{c|ccccc}
%		\toprule[1.5pt]
%		\hline
		\hline
		特征维度 & MOTA $\uparrow$ & MOTP $\uparrow$ & IDF1 $\uparrow$ & ID\_Sw $\downarrow$ & FPS$ \uparrow$\\
		\hline
%		\midrule[1.5pt]
		1024 & 55.3 & 76.3 & 57.3 & 8,107 & 15.3\\
		512 & 54.7 & 75.1 & 57.1 & 6,810 & 18.7\\
		256 & 56.2 & 78.9 & {\bf59.7} & 7,657 & 19.6\\
		128 & {\bf58.1} & \bf{79.8} & 59.2 & {\bf6,129} & 21.7\\
		64 & 58.1 & 71.6 & 56.7 & 11,675 & {\bf21.9}\\
%		\bottomrule[1.5pt]
		\hline
%		\hline
	\end{tabular}
	\label{tab:jdan_dimension}
\end{table}





%此外，评估了整个训练过程，在 表~\ref{tab:jdan_training_methods} 中命名为 JDAN。
%可以看出，所提出的方法的所有指标都比其他方法更好。
%这种增强不仅归功于微调的检测子模块，还归功于经过训练的关联子模块。
%例如，JDAN 的 MOTA 从 $53.0\%$ 提高到 $58.1\%$。
%特别是 JDAN 表现出最好的 MOTA 性能，因为它获得了强大的目标关联能力。
%此外，JDAN 比“未精调关联网络”具有更强的联想能力。
%因此，认为在 JDAN 中利用关联训练是性能改进的主要来源，因为它可以提高多目标跟踪中的目标关联能力。


\subsubsection{目标表征维度} \label{sec:dimension}
目前已有的行人重新识别方法通常利用高维的目标表征，例如 $1024$，并在没有探究表征维度这个超参数的情况下在数据集上获得了优异的性能。
直到 FairMOT~\cite{fairmot} 发现了表征维度在目标跟踪中起着重要作用。
因为重识别数据集中缺少原始视频帧，所以多目标跟踪方法无法利用它。
故合适的低维目标表征在多目标跟踪中有更好的性能，因为与重新识别任务相比，多目标跟踪缺乏足够的公共训练数据集。
提取低维表征减轻了较小数据集的模型欠拟合问题，并提高多目标跟踪性能。
已有的两阶段多目标跟踪方法不受数据不足的影响，因为它们可以利用丰富的只提供裁剪行人图像的重新识别数据。
而一阶段多目标跟踪方法需要原始未裁剪的视频帧，它无法利用这些重新识别数据，
解决这个数据依赖问题的一种方法是降低目标表征的维数。

在表~\ref{tab:jdan_dimension} 中测试各种维度配置，可以看出当维度从 $1024$ 减少到 $128$ 时，MOTA 不断增加，这证明了多目标跟踪训练数据中低维表征的优点。
此外，当维数减少到 64 时，MOTA 开始减少，因为过低的目标表征已开始导致表征受损。
尽管 MOTA 分数的改进很小，而 ID\_Sw 改进了很多，从原先的 $8107$ 减少到 $6129$，这在提高整体多目标跟踪性能方面起着重要作用。
通过减少目标表征维度，模型运行速度也略有提高。
然而，只有在缺乏训练数据集时，使用低维目标表征才有效。
随着训练数据集的增加，可以缓解表征维度带来的问题。


\vspace{1.0em}
\renewcommand\arraystretch{1.5}
\begin{table}[htbp]\wuhao
	\centering
	\caption{最大目标数阈值 $N_m$ 对目标关联性能的影响}
	\vspace{0.3em}
	\begin{tabular}
%		{
%			p{2.0cm}<{\centering}
%			p{1.5cm}<{\centering}
%			p{1.3cm}<{\centering}
%			p{1.3cm}<{\centering}
%			p{1.3cm}<{\centering}
%			p{1.3cm}<{\centering}
%		}
		{c|ccccc}
%		\hline
		\hline
%		\toprule[1.5pt]
		特征维度 & Det-1024 & Det-512 & Det-256 & Det-128 & Det-64 \\
		\hline
%		\midrule[1.5pt]
		MOTA$^S$ & 56.3 & 55.7 & \bf 57.8 & 58.1 & 53.6 \\
		MOTA$^M$ & \bf{57.8} & \bf 57.9 & 55.3 & \bf 58.4 & \bf 55.3 \\
		MOTA$^L$ & 52.1 & 56.0 & 52.8 & 49.8 & 47.3 \\
		\hline
		ID\_Sw$^S$ & 8,519 & \bf 7,236 & \bf 7,083 & \bf 6,129 & 8,913 \\
		ID\_Sw$^M$ & \bf 2,107 & 8,013 & 7,657 &  6,346 & \bf 6,675 \\
		ID\_Sw$^L$ & 10,397 & 9,281 & 9,597 &  7,475 & 7,286 \\
%		\bottomrule[1.5pt]
		\hline
%		\hline
	\end{tabular}
	\label{tab:jdan_max_obj}
\end{table}




\subsubsection{最大目标数} \label{sec:maximum_object}
由于各种多目标跟踪环境中的目标密度不同，利用适当的最大目标数 $N_m$ 来适应不同的多目标跟踪环境。
与章节~\ref{sec:dimension} 类似，从表~\ref{tab:jdan_max_obj} 中最大目标数的各种配置中可以发现它会影响多目标跟踪性能。
上标~{S} 表示最大目标数为 100； 上标~{M} 表示最大目标数为 150； 上标~{L} 表示最大目标数为 200。
%{Feature dim} 是目标表征维度。
%在加粗中展示了最佳的多目标跟踪性能。
发现当根据章节~\ref{sec:dimension} 将最大目标数设置为 $ 150 $ 且目标表征维度设置为 $128$ 时，可以获得更好的结果。
可以看出过大的最大目标数将导致关联子模块中的欠拟合并降低多目标跟踪性能。





%\vspace{1.0em}
%\renewcommand\arraystretch{1.5}
%\begin{table}[htbp]\wuhao
%	\centering
%	\caption{提出的方法在 MOT15 和 MOT17 基准上与最新的一阶段方法比较}
%	\vspace{0.3em}
%	\begin{tabular}{p{2.0cm}<{\centering} p{1.8cm}<{\centering} p{1.3cm}<{\centering} p{1.2cm}<{\centering}p{1.3cm}<{\centering}p{1.2cm}<{\centering}}
%		\toprule[1.5pt]
%		Benchmark & Tracker & IDF1$\uparrow$ & MOTA$\uparrow$ & ID\_Sw$\downarrow$ & Hz$\uparrow$\\
%		\midrule[1.0pt]
%		MOT15 & JDE \cite{jde} & \bf 66.7 & \bf 67.5 & \bf 218 &  22.5\\
%		& \emph{JDAN}(ours) & { 61.8} & {57.8} & { 494} & {\bf 23.5}\\
%		\hline
%		MOT17 & JDE \cite{jde} & 55.8 & {\bf 64.4} & \bf 1,544 & 18.5\\
%		& \emph{JDAN}(ours) & {\bf 59.2} & 58.1 & {6,129} & {\bf 21.7}\\
%		\bottomrule[1.5pt]		
%	\end{tabular}
%	\label{tab:jdan_onestage}
%\end{table}


\vspace{0.5em}
\renewcommand\arraystretch{1.5}
\begin{table}[htbp]\wuhao
	\centering
	\caption{与其他在线多目标跟踪方法进行比较}
	\vspace{0.3em}
	\begin{tabular}{c|c|cccccc}
%		\hline
		\hline
		数据集 & 方法 & IDF1$\uparrow$ & MOTA$\uparrow$ & MT$\uparrow$ & ML$\downarrow$ & ID\_Sw$\downarrow$ & FPS$\uparrow$\\
		\hline
		MOT15 
		& MDP\_SubCNN\cite{xiang2015learning} & 55.7 & 47.5 & 30.0\% & 18.6\% & 628 & 2.1\\
		& CDA\_DDAL\cite{bae2017confidence} & 54.1 & 51.3 & 36.3\% & 22.2\% & 544 & 1.3\\
		& EAMTT\cite{sanchez2016online} & 54.0 & 53.0 & 35.9\% & 19.6\% & 7,538 & 11.5\\ 
		& AP\_HWDPL\cite{chen2017online} & 52.2 & 53.0 & 29.1\% & 20.2\% & 708 & 6.7\\
		& RAR15\cite{fang2018recurrent} & 61.3 & 56.5 & 45.1\% & 14.6\% & {\bf 428} & 5.1\\
		& {JDE\cite{jde}\textsuperscript{*}} & {56.9} & {{\bf 62.1}} & {34.4\%} & {16.7\%} & {1,608} & {22.5}\\
		& JDAN\textsuperscript{*} & {\bf 61.8} & 57.8 & {\bf 45.3\%} & {\bf 12.9\%} & 494 & {\bf 23.5}\\
		\hline
		MOT17 
		& DMAN~\cite{dual_matching} & 55.7 & 48.2 & 19.3\% & 38.3\% & 2,193 & 0.3\\
		& MTDF~\cite{gm_phd} & 45.2 & 49.6 & 18.9\% & 33.1\% & 5,567 & 1.3\\
		& FAMNet~\cite{famnet} & 48.7 & 52.0 & 19.1\% & 33.4\% & 3,072 & 0.6\\
		& Tracktor++~\cite{tracktor} & 52.3 & 53.5 & 19.5\% & 36.6\% & 2,072 & 2.0\\
		& SST\cite{dan} & 49.5 & 52.4 & 21.4\% & 30.7\% & 8,431 & 6.3\\
		& {JDE\cite{jde}\textsuperscript{*}} & {55.8} & {{\bf 64.4}} & {{\bf 32.8\%}} & {{\bf 17.9\%}} & {{\bf 1,544}} & {18.5}\\
		& JDAN\textsuperscript{*} & {\bf 59.2} & 58.1 & 27.7\% & 32.9\% & {6,129} & {\bf 21.7}\\
		\hline
%		\hline
	\end{tabular}
	\label{tab:jdan_sota}
\end{table}


\vspace{1.0em}
\renewcommand\arraystretch{1.5}
\begin{table}[htbp]\wuhao
	\centering
	\caption{在 MOT17 上每个视频中跟踪效果的详细信息}
	\vspace{0.3em}
	\begin{tabular}{c|ccccccc}
%	\begin{tabular}{p{2.5cm}<{\centering} | p{1.2cm}<{\centering} p{1.2cm}<{\centering} p{1.2cm}<{\centering}p{1.3cm}<{\centering}p{1.2cm}<{\centering}p{1.2cm}<{\centering}p{1.2cm}<{\centering}}
%		\hline
		\hline
		视频序列 & MOTA$\uparrow$ & IDF1$\uparrow$ & MOTP$\uparrow$ & MT$\uparrow$ & ML$\downarrow$ & FP$\downarrow$ & FN$\downarrow$\\
		\hline
%		\midrule[1.0pt]
		MOT17-01 & 55.57 & 53.88 & 80.49 & 33.33\% & 20.83\% & 249 & 2,507\\
		MOT17-03 & 77.09 & 73.35 & 80.69 & 75.68\% & 7.43\% & 10,102 & 13,533\\
		MOT17-06 & 26.68 & 18.91 & 64.54 & 2.25\% & 62.16\% & 6,017 & 8,464\\
		MOT17-07 & 51.53 & 41.13 & 80.60 & 20.67\% & 18.33\% & 512 & 5,767\\
		MOT17-08 & 34.49 & 29.95 & 81.18 & 19.74\% & 38.16\% & 287 & 12,706\\
		MOT17-12 & 50.90 & 54.45 & 80.08 & 28.57\% & 28.57\% & 792 & 3,310\\
		MOT17-14 & 41.83 & 41.56 & 75.13 & 23.17\% & 21.95\% & 739 & 7,696\\
		\hline
		所有 & 58.05 & 59.22 & 79.84 & 27.73\% & 32.99\% & 18,698 & 53,983\\
%		\hline
		\hline	
	\end{tabular}
	\label{tab:jdan_mot17_detailed}
\end{table}



\subsection{性能比较}
在这一部分中，通过与现有的一阶段方法和两阶段方法在内的多目标跟踪方法进行比较来评估和分析所提出的方法。


\subsubsection{两阶段多目标跟踪方法}
本节通过与许多现有的在线多目标跟踪方法进行性能比较来测试和分析 {JDAN}。
为了证明所提出的端到端检测跟踪方法的有效性，在表~\ref{tab:jdan_sota} 中包含了一些关于 MOT15 和 MOT17 的一阶段和两阶段方法。
值得注意的是，两阶段多目标跟踪方法中的跟踪速度（Hz）只包含跟踪阶段而不包括的检测阶段的时间。
然而，在一阶段方法的测试中，所花费的时间同时包括检测和关联。
请注意，用“*”标记了一阶段多目标跟踪方法。
%最好的结果显示在加粗中。
因为不使用多目标跟踪基准数据集中提供的边界框，所以使用了私有检测器。
在 MOT15 和 MOT17 中的测试数据集上展示了多目标跟踪性能。
在评估性能之前，对训练后的模型进行了 6 个迭代轮次的微调。

在 MOT15 和 MOT17 数据上，所提出算法的性能优于其他在线多目标跟踪方法。
与之前的多目标跟踪方法相比，所提出的算法在两个多目标跟踪基准数据上获得了最好的 MOTA 分数，这显示了所提出的方法拥有出色的 MOT 性能，并且所提出的方法计算效率也非常高，所提出的方法获得了接近帧率的跟踪速度。
相比之下，许多高性能方法，例如~\cite{fang2018recurrent,poi}，其预测速度低于本章所提出的方法。


\subsubsection{一阶段多目标跟踪方法}
在之前的研究中，只有 TrackR-CNN~\cite{voigtlaender2019mots}、JDE~\cite{jde} 和 FairMOT~\cite{fairmot} 都使用了行人检测和表征学习。
但是 TrackR-CNN 需要额外的图像分割真实标签，并在图像分割问题中使用不同的方法衡量跟踪性能。
%此外，由于使用传统跟踪生成关联数据并训练的关联子模块，因此跟踪精度自然会低于之前开发的方法。
因此，在研究中，将所提出的方法与 JDE 进行比较，只是为了证明提出的端到端检测跟踪方法的有效性。

为了进行合理的评估，使用了 JDE~\cite{jde} 中用于描述的类似数据集，但是没有使用 MOT16 数据集，因为它与 MOT17 具有相同的视频序列，并且它包含的原始视频帧和 MOT16 没有区别。
还利用了 IDF1~\cite{ristani2016performance} 和 CLEAR 指标~\cite{bernardin2008evaluating} 来评估跟踪性能，跟踪性能见表~\ref{tab:jdan_sota}。
容易看出在 MOT17 数据上，所提出的方法优于 JDE~\cite{jde}。
并且它将 IDF1 从 $55.8$ 上升到 $59.2$，整体提高了多目标跟踪性能，详细地跟踪效果信息如图~\ref{tab:jdan_mot17_detailed} 所示。
多目标跟踪性能证明了一阶段方法的优势。
此外，多目标跟踪速度是该算法进行实际应用的一个非常重要优势。



\section{本章小结}
在本章中设计了一个端到端的框架 JDAN 来缓解多目标跟踪任务的误差传播问题，它可以联合训练检测和关联任务，将目标检测和跟踪同时进行优化，超越现有方法。
从技术实现上讲，使用伪标签来解决目标不一致问题，并设计了一个{连接子模块}以及一个{关联预测器}来产生精确的跟踪结果。
{端到端}的架构非常简单而有效，与现有的多目标跟踪方法相比，它避免了繁琐的手动设置。
在广泛使用的多目标跟踪数基准数据集上进行的一系列实验证明了所提出的方法在精度和效率方面都有很大的优势。
相信这个工作可以启发和激励端到端多目标跟踪任务的进一步研究。

%在本章中，设计了一种端到端方法来联合解决目标检测和多目标跟踪任务。
%超越现有的 JDE 方法~\cite{jde}，将目标检测和关联结合到一个单一的神经网络中。
%特别是，负责区分不同目标的目标表征嵌入被提取两次，导致两阶段多目标跟踪的重复计算。
%该工作的实现放弃了目标检测和跟踪中的显式锚点，从而提高了多目标跟踪的性能。
%此外，展示了一种明确且自然的端到端 方法，该方法直接利用目标表征将不同帧中的目标关联起来，
%解决端到端检测跟踪中{检测子模块}的输出与{关联子模块}的输入不一致的问题。
%最后，对广泛使用的多目标跟踪基准进行的大量实验证明了所提出的方法在有效性和效率方面的优越性。
%相信所提出的 {JDAN} 可以鼓励和激发一阶段多目标跟踪任务的新方法。
